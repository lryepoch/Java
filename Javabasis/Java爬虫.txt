
·Jsoup：一款Java的HTML解析器

    /**
     * 获取页面文档数据
     */
    Document doc = Jsoup.connect(url).get();

    /**
     * 获取页面上所有的tbody标签
     */
    Elements elements = doc.getElementsByTag("tbody");
    /**
     * 拿到第五个tbody标签
     */
    Element element = elements.get(4);

    /**
     * 拿到第五个tbody标签下所有的子标签
     */
    Elements childrens = element.children();



·HttpClient：获取页面，然后利用选择器解析得到数据。与正则表达式结合使用。





------------------------------
·框架：webmagic


·解析这个节点信息呢？方法有千千万万，经常使用的选择器应该是 CSS 选择器 和 XPath ，如果你还不知道这两种选择器，可以点击下方链接学习了解一下：
	CSS 选择器参考手册：https://www.w3school.com.cn/cssref/css_selectors.asp
	XPath 教程：https://www.w3school.com.cn/xpath/xpath_syntax.asp

	使用 CSS 选择器解析的写法为：#wgt-ask > h1 > span
	使用  XPath 解析的写法为：//span[@class="wgt-ask"]





-----------------------------

·httpclient + 正则表达式








----------------------------
·需要登陆的问题主要有两种解决方式：

	··一种方式是手动设置 cookie ，就是先在网站上面登录，复制登陆后的 cookies ，在爬虫程序中手动设置 HTTP 请求中的 Cookie 属性，这种方式适用于采集频次不高、采集周期短，因为 cookie 会失效，如果长期采集的话就需要频繁设置 cookie，这不是一种可行的办法；

	··第二种方式就是使用程序模拟登陆，通过模拟登陆获取到 cookies，这种方式适用于长期采集该网站，因为每次采集都会先登陆，这样就不需要担心 cookie 过期的问题。


----------------------------
·爬虫遇上数据异步加载：
	··内置一个浏览器内核：
		内置浏览器就是在抓取的程序中，启动一个浏览器内核，使我们获取到js渲染后的页面，这样我们就跟采集静态页面一样了。这种工具常用的有以下三种：
		Selenium
		HtmlUnit
		PhantomJs
		这些工具都能帮助我们解决数据异步加载的问题，但是他们都存在缺陷，那就是效率不高而且不稳定。

	··反向解析法：
		什么是反向解析法呢？我们js渲染页面的数据是通过Ajax的方式从后端获取的，我们只需要找到对应的Ajax请求连接就OK，这样我们就获取到了我们需要的数据，反向解析法的好处就是这种方式获取的数据都是json格式的数据，解析起来也比较方便，另一个好处就是相对页面来说，接口的变化概率更小。同样它有两个不足之处，一个是在Ajax时你需要有耐心有技巧，因为你需要在一大推请求中找到你想要的，另一个不足的地方就是对JavaScript渲染的页面束手无策。

